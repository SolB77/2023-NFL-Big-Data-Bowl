{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2023 NFL Big Data Bowl**\n",
    "### Sol Ben-Ishay"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ML Models**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **General**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run feature_engineering.ipynb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Import ML Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ML Libraries\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "from sklearn import tree\n",
    "from sklearn import neighbors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **ML Utilities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train/test/evaluate a single classification model\n",
    "def run_experiment(model, X_train, y_train, X_test, y_test, show_dt):\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot()\n",
    "    plt.show()\n",
    "    if show_dt:\n",
    "        tree.plot_tree(model)\n",
    "\n",
    "    print('Precision: %.3f' % precision_score(y_test, y_pred))\n",
    "    print('Recall: %.3f' % recall_score(y_test, y_pred))\n",
    "    print('F1: %.3f' % f1_score(y_test, y_pred))\n",
    "    print('Accuracy: %.3f' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train/test/evaluate multiple classification models\n",
    "def run_experiments(models, X_train, y_train, X_test, y_test):\n",
    "    with plt.ioff():\n",
    "        # Create the performance results dictionary and main plot\n",
    "        results = {\"Model Name\":[],\"Precision\":[],\"Recall\":[],\"F1\":[],\"Accuracy\":[]}\n",
    "        confusion_matrices, axs = plt.subplots(nrows=2, ncols=2, figsize=(7, 7))\n",
    "        confusion_matrices.suptitle(\"Model Confusion Matrices\")\n",
    "        confusion_matrices.tight_layout()\n",
    "\n",
    "        for model_name, ax in zip(list(models.keys()), axs.ravel()):\n",
    "            # Train/test the model\n",
    "            model = models[model_name]\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            # Create a confusion matrix\n",
    "            cm = confusion_matrix(y_test, y_pred)\n",
    "            disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "            disp.plot(ax=ax)\n",
    "            ax.title.set_text(model_name)\n",
    "\n",
    "            # Append the performance results\n",
    "            results['Model Name'].append(model_name)\n",
    "            results['Precision'].append(precision_score(y_test, y_pred))\n",
    "            results['Recall'].append(recall_score(y_test, y_pred))\n",
    "            results['F1'].append(f1_score(y_test, y_pred))\n",
    "            results['Accuracy'].append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "        # Show the performance metrics and plot the confusion matrices\n",
    "        print(\"Experiment Results:\")\n",
    "        results_df = pd.DataFrame(results).style.set_caption(\"Model Performance Metrics\")\n",
    "        display(results_df)\n",
    "        print()\n",
    "        plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model A. Predict whether a DL will get double teamed on a play (pre-snap)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model B. Predict whether a DL will get a pressure on a play (pre-snap)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model C. Predict whether a DL will get a pressure on a play (intra-play)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Feature Engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the relevant data for the model(s)\n",
    "# Note: Only training/testing on players who pass rushed more than 35 snaps and for plays their role was \"Pass Rush\"\n",
    "# Note: potentially consider OLB in DL\n",
    "# Get the high snap pass rushers\n",
    "high_snap_pass_rushers = player_level_df.query(\"officialPosition in ['NT','DT','DE'] and pff_role == 'Pass Rush'\").nflId.value_counts().loc[lambda c: c > 35].index.tolist()\n",
    "\n",
    "# Get all the relevant model data (Don't include game/play/player info)\n",
    "player_level_coi = ['gameId', 'playId', 'nflId', 'displayName',\n",
    "        'officialPosition', 'pff_role', 'pff_positionLinedUp', 'pff_pressure',\n",
    "        # 'pff_hit', 'pff_hurry', 'pff_sack'\n",
    "        ]\n",
    "play_level_coi = ['gameId','playId','passResult','defendersInBox', 'down', 'yardsToGo']\n",
    "tracking_coi = ['gameId','playId','nflId','speed_at_1.5','depth_at_1.5','distance_at_1.5']\n",
    "model_c_df = (player_level_df[player_level_coi].query(f\"nflId in {high_snap_pass_rushers} and pff_role == 'Pass Rush'\")\n",
    "        .merge(play_level_df[play_level_coi], how='left', on=['gameId','playId'])\n",
    "        .merge(dl_alignment_df, how='left', on=['gameId','playId','nflId'])\n",
    "        .merge(dl_num_blockers_df, how='left', on=['gameId','playId','nflId'])\n",
    "        # .merge(time_to_depth_df, how='left', on=['gameId','playId','nflId'])\n",
    "        .merge(pass_rushers_at_key_df[tracking_coi], how='left', on=['gameId','playId','nflId'])\n",
    "        # .merge(pass_rushers_at_key2_df, how='left', on=['gameId','playId','nflId'])\n",
    "        # .merge(rusher_dist_at_action_df, how='left', on=['gameId','playId','nflId'])\n",
    "        .merge(snap_to_action_df, how='left', on=['gameId','playId'])\n",
    "        .drop(columns=['gameId', 'playId', 'nflId', 'displayName','pff_role']))\n",
    "\n",
    "## Clean up NAs\n",
    "# A missing snap_to_action_time/distance at rel occurs when a certain play is missing either a snap or action event\n",
    "# Investigate missing dl_alignment/distance at 1.5\n",
    "model_c_df = model_c_df.dropna(subset=['dl_alignment','distance_at_1.5','defendersInBox'], how='any')\n",
    "# numBlockers and multBlockers is N/A when a player is unblocked\n",
    "model_c_df = model_c_df.fillna(value={'numBlockers':0,'multBlockers':'U'})\n",
    "\n",
    "## Filter plays\n",
    "# Only look at plays that were either C, I, S, or IN (no scrambles, laterals, etc)\n",
    "model_c_df = model_c_df.query(\"passResult in ['C','I','S','IN','R']\").drop(columns=[\"passResult\"])\n",
    "#  Only look at plays that have a snap to action time within the 95th percentile\n",
    "model_c_df = (model_c_df[model_c_df.snap_to_action_time < model_c_df.snap_to_action_time.quantile(.95)]\n",
    "        .drop(columns=[\"snap_to_action_time\"]))\n",
    "\n",
    "model_c_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NAs\n",
    "# A missing time_to_depth occurs when the rusher did not make it to 3 yards\n",
    "# Missing metrics at 2 seconds indicate plays that did not last 2 seconds (maybe should filter out?)\n",
    "if sum(model_c_df.isna().sum().values) != 0:\n",
    "    print(model_c_df.isna().sum().loc[lambda x: x > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the categorical data\n",
    "\n",
    "# Official position\n",
    "one_hot = pd.get_dummies(model_c_df['officialPosition'], prefix=\"pos_\")\n",
    "model_c_df = model_c_df.join(one_hot)\n",
    "\n",
    "# Position lined up\n",
    "model_c_df.loc[model_c_df['pff_positionLinedUp'].isin(['NT','NLT','NRT']), 'pff_positionLinedUp'] = 0\n",
    "model_c_df.loc[model_c_df['pff_positionLinedUp'].isin(['DLT']), 'pff_positionLinedUp'] = 1\n",
    "model_c_df.loc[model_c_df['pff_positionLinedUp'].isin(['DRT']), 'pff_positionLinedUp'] = 2\n",
    "model_c_df.loc[model_c_df['pff_positionLinedUp'].isin(['LE']), 'pff_positionLinedUp'] = 3\n",
    "model_c_df.loc[model_c_df['pff_positionLinedUp'].isin(['RE']), 'pff_positionLinedUp'] = 4\n",
    "model_c_df.loc[model_c_df['pff_positionLinedUp'].isin(['LEO','LOLB']), 'pff_positionLinedUp'] = 5\n",
    "model_c_df.loc[model_c_df['pff_positionLinedUp'].isin(['REO','ROLB']), 'pff_positionLinedUp'] = 6\n",
    "model_c_df.loc[model_c_df['pff_positionLinedUp'].isin(['LILB','RILB','MLB','LLB','RLB']), 'pff_positionLinedUp'] = 7\n",
    "model_c_df.loc[~model_c_df['pff_positionLinedUp'].isin([0,1,2,3,4,5,6,7]), 'pff_positionLinedUp'] = 8\n",
    "model_c_df['pff_positionLinedUp'] = model_c_df['pff_positionLinedUp'].astype(\"int\")\n",
    "\n",
    "one_hot = pd.get_dummies(model_c_df['pff_positionLinedUp'], prefix=\"posLinedUp_\")\n",
    "model_c_df = model_c_df.join(one_hot)\n",
    "\n",
    "# DL alignment\n",
    "one_hot = pd.get_dummies(model_c_df['dl_alignment'], prefix=\"alignment_\")\n",
    "model_c_df = model_c_df.join(one_hot)\n",
    "\n",
    "# Multiple blockers\n",
    "# one_hot = pd.get_dummies(model_c_df['multBlockers'], prefix=\"multBlockers_\")\n",
    "# model_c_df = model_c_df.join(one_hot)\n",
    "\n",
    "model_c_df = model_c_df.drop(columns=[\"officialPosition\",\"pff_positionLinedUp\",\"dl_alignment\",\"multBlockers\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the numerical data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "coi = ['defendersInBox','down','yardsToGo','numBlockers','speed_at_1.5','depth_at_1.5','distance_at_1.5']\n",
    "model_c_df[coi] = scaler.fit_transform(model_c_df[coi])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_c_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Feature Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature/target descriptive stats\n",
    "# Note: Check for only plays they got pressures to see if there is a visually evident relationship with any features\n",
    "num_coi = [\"pff_pressure\", \"distance_at_1.5\", \"depth_at_1.5\", \"speed_at_1.5\", \"numBlockers\",'defendersInBox']\n",
    "model_c_df[num_coi].describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature/target means by player\n",
    "# coi = ['nflId', 'displayName', 'officialPosition', 'pff_hit', 'pff_hurry', 'pff_sack', 'pff_pressure', 'distance_at_1.5', 'depth_at_1.5', 'speed_at_1.5', 'numBlockers']\n",
    "\n",
    "# indy_dl_avg_df = model_c_df[coi].groupby(['nflId', 'displayName', 'officialPosition']).mean().reset_index()\n",
    "\n",
    "# indy_dl_avg_df.sort_values(by=['distance_at_2','speed_at_2'], ascending=[True, True]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature descriptive stats by target (pff_pressure)\n",
    "model_c_df[num_coi].groupby('pff_pressure').describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two-way frequency table (multiple blockers and pff pressure)\n",
    "# pd.crosstab(index=model_c_df['multBlockers'], columns=model_c_df['pff_pressure'], normalize='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "corr = (model_c_df.corr(numeric_only=True))\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "figure = plt.figure(figsize=[8,8]).suptitle(\"Correlation Heatmap\")\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Modeling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the model data\n",
    "\n",
    "# Model data w N/As dropped\n",
    "model_c_df = model_c_df.dropna()\n",
    "\n",
    "# Features\n",
    "features = model_c_df.drop(columns=['pff_pressure'])\n",
    "\n",
    "# Target\n",
    "target = model_c_df['pff_pressure']\n",
    "\n",
    "# Train/test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, target, test_size = 0.2, shuffle=True)\n",
    "\n",
    "# Balance the sample of training data for the outcome class (with/without a pressure)\n",
    "under_sampler = RandomUnderSampler(random_state=77)\n",
    "x_train, y_train = under_sampler.fit_resample(x_train, y_train)\n",
    "print(f\"Training target statistics: {Counter(y_train)}\")\n",
    "print(f\"Testing target statistics: {Counter(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the experiments\n",
    "decision_tree_models = {\"Decision Tree\":tree.DecisionTreeClassifier()}\n",
    "knn_models = {\"kNN (k=5)\":neighbors.KNeighborsClassifier(n_neighbors=5),\n",
    "            \"kNN (k=7)\":neighbors.KNeighborsClassifier(n_neighbors=7),\n",
    "            \"kNN (k=9)\":neighbors.KNeighborsClassifier(n_neighbors=9)}\n",
    "models = decision_tree_models | knn_models\n",
    "\n",
    "run_experiments(models, x_train, y_train, x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('kaggle_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7f1b956008a91c55b58e943197941d68155f979680d4b59411e907f4e002aa97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
